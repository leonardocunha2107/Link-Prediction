{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "link_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4ezpSx3UQ0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install torch_geometric\n",
        "!pip install torch_sparse\n",
        "!pip install torch_scatter\n",
        "!pip install torch_cluster\n",
        "!pip install torch-spline-conv\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMKFY_L5pjx7",
        "colab_type": "code",
        "outputId": "8d614d5b-e241-46e4-8dc6-c1b777c8acbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!git clone https://github.com/leonardocunha2107/Link-Prediction\n",
        "!rm -rf node_information\n",
        "!mv Link-Prediction/* .\n",
        "!rm -rf Link-Prediction\n",
        "\n",
        "num_nodes=33226"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Link-Prediction'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 47 (delta 17), reused 38 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaRZp_FmfhaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def edge_tensors(val_cut=0.8,device=torch.device(\"cuda\")):\n",
        "    neg_edges=[]\n",
        "    pos_edges=[]\n",
        "    ypos,yval=[]\n",
        "    with open(\"training.txt\", \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.split()\n",
        "            edge=[int(line[0]),int(line[1])]\n",
        "            if line[2] == '1':\n",
        "                pos_edges.append(edge)\n",
        "            else: neg_edges.append(edge)\n",
        "    random.shuffle(pos_edges)\n",
        "    random.shuffle(neg_edges)\n",
        "    pos_edges,neg_edges=np.array(pos_edges),np.array(neg_edges)\n",
        "    postrain_edges,negtrain_edges=pos_edges[:int(len(pos_edges)*val_cut)],neg_edges[:int(len(neg_edges)*val_cut)]\n",
        "    posval_edges,negval_edges=pos_edges[int(len(pos_edges)*val_cut):],neg_edges[int(len(neg_edges)*val_cut):]\n",
        "    val_edges=np.vstack([posval_edges,negval_edges])\n",
        "    \n",
        "    edge1,edge2=np.hstack([postrain_edges[:,0],postrain_edges[:,1]]),np.hstack([postrain_edges[:,1],postrain_edges[:,0]])\n",
        "    postrain_edges_tensor=torch.tensor(np.vstack([edge1,edge2]),device=device,dtype=torch.long)\n",
        "    \n",
        "    edge1,edge2=np.hstack([negtrain_edges[:,0],negtrain_edges[:,1]]),np.hstack([negtrain_edges[:,1],negtrain_edges[:,0]])\n",
        "    negtrain_edges_tensor=torch.tensor(np.vstack([edge1,edge2]),device=device,dtype=torch.long)\n",
        "    \n",
        "    edge1,edge2=np.hstack([val_edges[:,0],val_edges[:,1]]),np.hstack([val_edges[:,1],val_edges[:,0]])\n",
        "    valedges_tensor=torch.tensor(np.vstack([edge1,edge2]),device=device,dtype=torch.long)\n",
        "    \n",
        "    ytrain=torch.tensor([1]*(postrain_edges_tensor.shape[1]))\n",
        "    return postrain_edges_tensor,\\\n",
        "            torch.cat([postrain_edges_tensor,negtrain_edges_tensor],axis=1),valedges_tensor\n",
        "\n",
        "n2v_tensor,train_data,val_data=edge_tensors()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD_R7Ro5TE1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "18f51d60-dd54-484e-8de8-223a43e6cd9f"
      },
      "source": [
        "print(n2v_tensor.shape)\n",
        "print(val_tensor.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 453796])\n",
            "torch.Size([2, 181520])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNRFy8cmV7n0",
        "colab_type": "code",
        "outputId": "8dba9d5e-1b2b-4e02-8fdf-216929fbeba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "#from data import  get_edge_tensor\n",
        "from torch_geometric.nn.models import Node2Vec\n",
        "#from node2vec import Node2Vec\n",
        "device  = torch.device(\"cuda\")\n",
        "d=128\n",
        "num_nodes=33226\n",
        "n2v=Node2Vec(num_nodes,d,5,3,walks_per_node=3).to(device)\n",
        "#n2v=Node2Vec(num_nodes,d,5,3,'text_embeds.npy',walks_per_node=2).to(device)\n",
        "optimizer=torch.optim.Adam(n2v.parameters(),lr=1e-1)\n",
        "n2v_tensor=n2v_tensor.to(device)\n",
        "#print(edges_tensor[:,:10])\n",
        "for i in range(100):\n",
        "    \n",
        "    loss=n2v.loss(n2v_tensor)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward(retain_graph=False)\n",
        "    optimizer.step()\n",
        "    print(f\"Loss {loss.data}\")\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss 9.863016128540039\n",
            "Loss 8.520421981811523\n",
            "Loss 7.550008296966553\n",
            "Loss 6.841428756713867\n",
            "Loss 6.244897842407227\n",
            "Loss 5.7738237380981445\n",
            "Loss 5.383397579193115\n",
            "Loss 5.014370441436768\n",
            "Loss 4.716630935668945\n",
            "Loss 4.450439929962158\n",
            "Loss 4.177728176116943\n",
            "Loss 3.9507694244384766\n",
            "Loss 3.753190755844116\n",
            "Loss 3.5460922718048096\n",
            "Loss 3.367779493331909\n",
            "Loss 3.193480968475342\n",
            "Loss 3.0441927909851074\n",
            "Loss 2.90509295463562\n",
            "Loss 2.7736263275146484\n",
            "Loss 2.6626944541931152\n",
            "Loss 2.553676128387451\n",
            "Loss 2.4445512294769287\n",
            "Loss 2.3356692790985107\n",
            "Loss 2.242884397506714\n",
            "Loss 2.1573426723480225\n",
            "Loss 2.0761501789093018\n",
            "Loss 2.000098943710327\n",
            "Loss 1.9217042922973633\n",
            "Loss 1.8603825569152832\n",
            "Loss 1.7968628406524658\n",
            "Loss 1.7407807111740112\n",
            "Loss 1.6881300210952759\n",
            "Loss 1.6309468746185303\n",
            "Loss 1.5865185260772705\n",
            "Loss 1.54324471950531\n",
            "Loss 1.5006697177886963\n",
            "Loss 1.4627289772033691\n",
            "Loss 1.4317281246185303\n",
            "Loss 1.3913064002990723\n",
            "Loss 1.3660449981689453\n",
            "Loss 1.337429404258728\n",
            "Loss 1.304802656173706\n",
            "Loss 1.2832257747650146\n",
            "Loss 1.2561067342758179\n",
            "Loss 1.233104944229126\n",
            "Loss 1.2159783840179443\n",
            "Loss 1.1965687274932861\n",
            "Loss 1.1771166324615479\n",
            "Loss 1.1621934175491333\n",
            "Loss 1.1446858644485474\n",
            "Loss 1.130476951599121\n",
            "Loss 1.1147491931915283\n",
            "Loss 1.1049243211746216\n",
            "Loss 1.0922281742095947\n",
            "Loss 1.081695556640625\n",
            "Loss 1.0725871324539185\n",
            "Loss 1.061218023300171\n",
            "Loss 1.0517579317092896\n",
            "Loss 1.0445133447647095\n",
            "Loss 1.036568284034729\n",
            "Loss 1.027431845664978\n",
            "Loss 1.021059513092041\n",
            "Loss 1.014148235321045\n",
            "Loss 1.0085358619689941\n",
            "Loss 1.0030813217163086\n",
            "Loss 0.9972820281982422\n",
            "Loss 0.9919725656509399\n",
            "Loss 0.987800121307373\n",
            "Loss 0.9834441542625427\n",
            "Loss 0.9787019491195679\n",
            "Loss 0.9740073680877686\n",
            "Loss 0.9712041616439819\n",
            "Loss 0.966551661491394\n",
            "Loss 0.9606397151947021\n",
            "Loss 0.9586475491523743\n",
            "Loss 0.9576930999755859\n",
            "Loss 0.9525266289710999\n",
            "Loss 0.9501424431800842\n",
            "Loss 0.9483751058578491\n",
            "Loss 0.9447083473205566\n",
            "Loss 0.9426136612892151\n",
            "Loss 0.9409314393997192\n",
            "Loss 0.9396529197692871\n",
            "Loss 0.9368672370910645\n",
            "Loss 0.9357694387435913\n",
            "Loss 0.932114839553833\n",
            "Loss 0.931086003780365\n",
            "Loss 0.9287736415863037\n",
            "Loss 0.9276385307312012\n",
            "Loss 0.9253186583518982\n",
            "Loss 0.9249606728553772\n",
            "Loss 0.924634575843811\n",
            "Loss 0.9225748777389526\n",
            "Loss 0.9209364652633667\n",
            "Loss 0.9202418327331543\n",
            "Loss 0.9192554354667664\n",
            "Loss 0.9186803102493286\n",
            "Loss 0.9170414209365845\n",
            "Loss 0.91740882396698\n",
            "Loss 0.9151856899261475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRSeTYdAs__F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class  TrainDS:\n",
        "    def __init__(self,embed):\n",
        "        self.embed=embed\n",
        "        self.edges=[]\n",
        "        self.y=[]\n",
        "        with open('training.txt','r') as fd:\n",
        "            for l in fd:\n",
        "                l=[int(x) for x in l.split()]\n",
        "                self.edges.append((l[0],l[1]))\n",
        "                self.y.append(l[2])\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self,idx):\n",
        "        edge=self.edges[idx]\n",
        "        x=torch.cat([self.embed[edge[0]],self.embed[edge[1]]])\n",
        "        return x,torch.tensor(self.y[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iB1jF4Iqwlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from data import TrainDS\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "embed=n2v(torch.arange(num_nodes,device=device)).data\n",
        "\"\"\"dataset=TrainDS(embed)\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.2 * dataset_size))\n",
        "random_seed=42\n",
        "np.random.seed(random_seed)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\"\"\"\n",
        "batch_size=128\n",
        "train_loader = torch.utils.data.DataLoader(train_tensor, batch_size=batch_size, \n",
        "                                           sampler=train_sampler)\n",
        "validation_loader = torch.utils.data.DataLoader(val_tensor,dataset, batch_size=batch_size,\n",
        "                                                sampler=valid_sampler)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGgPkR5VsyPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "\n",
        "def train_epoch(model,optim,train_dl,loss_fn=nn.CrossEntropyLoss()):\n",
        "    loss_avg=0\n",
        "    device=torch.device(\"cuda\")\n",
        "    model.train()\n",
        "    i=0\n",
        "    for x,y in train_dl:\n",
        "        optim.zero_grad()\n",
        "        x = x.to(device=device, dtype=torch.float)  \n",
        "        y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "        scores = model(x)\n",
        "        \n",
        "        loss = loss_fn(scores, y)\n",
        "        loss_avg+=loss.data\n",
        "        loss.backward(retain_graph=False)\n",
        "\n",
        "        optim.step()\n",
        "        i+=1\n",
        "\n",
        "    \n",
        "    print('Loss %.4f' %loss/i)\n",
        "    \n",
        "    return loss_avg\n",
        "def loader_stats(model,dataloader,threshold=0.5):\n",
        "    tp,fp,tn,fn=0,0,0,0\n",
        "    with torch.no_grad():\n",
        "        model.eval()  # set model to evaluation mode\n",
        "        \n",
        "        for x,y in dataloader:\n",
        "            x = x.to(device=device, dtype=torch.float)  \n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "            scores = model(x)\n",
        "            scores=torch.nn.functional(softmax(scores))\n",
        "            preds = (scores[:,1]>threshold).long()\n",
        "            comp=(preds==y).long()\n",
        "\n",
        "            ##calcule standard stats\n",
        "            tp+=(comp*y).sum().data\n",
        "            fp+=((1-comp)*(1-y)).sum().data\n",
        "            tn+=((comp)*(1-y)).sum().data\n",
        "            fn+=((1-comp)*y).sum().data\n",
        "    tp,fp,tn,fn=float(tp)+1e-5,float(fp)+1e-5,float(tn)+1e-5,float(fn)+1e-5\n",
        "    n=tp+fp+tn+fn\n",
        "    raw={'n':n,'tp':tp,'fp':fp,'tn':tn,'fn':fn}\n",
        "\n",
        "    pr,prec=tp/(tp+fn),tp/(tp+fp)\n",
        "    metrics={'acc':(tp+tn)/n,'pr':pr,'prec':prec,'nr':tn/(tn+fp),'f1':(2*prec*pr)/(prec+pr)}  \n",
        "\n",
        "    log={'raw':raw,'metrics':metrics}\n",
        "    pprint(log['metrics'])\n",
        "    return log\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY1mWXKI1LUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "class Classifier(nn.Module):\n",
        "    def  __init__(self,d):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1=nn.Linear(2*d,2*d)\n",
        "        self.bn1=nn.BatchNorm1d(2*d)\n",
        "        self.fc2=nn.Linear(2*d,2*d)\n",
        "        self.fc3=nn.Linear(2*d,2*d)\n",
        "        self.bn2=nn.BatchNorm1d(2*d)\n",
        "        self.fc4=nn.Linear(2*d,2*d)\n",
        "        self.fc5=nn.Linear(2*d,2)\n",
        "    def forward(self,x):\n",
        "        x=F.relu(self.fc1(x))\n",
        "        x=self.bn1(x)\n",
        "        x=F.relu(self.fc2(x))#+x\n",
        "        x=F.relu(self.fc3(x))\n",
        "        x=F.relu(self.fc4(x))+x\n",
        "        x=self.bn2(x)\n",
        "        x=F.relu(self.fc4(x))\n",
        "        return self.fc5(x)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unCCaJI85sTp",
        "colab_type": "code",
        "outputId": "7275956f-80a2-454f-a18e-4f65d496c6b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import utils\n",
        "classifier=nn.Sequential(nn.Linear(2*d,2*d),nn.ReLU(), nn.Linear(2*d,d),nn.ReLU(),nn.Linear(d,2)).cuda()\n",
        "#classifier=Classifier(d).to(device=device)\n",
        "optim=torch.optim.RMSprop(classifier.parameters(),lr=1e-4)\n",
        "#weight=torch.tensor([1,5],device=device).type(torch.float)\n",
        "\n",
        "for epoch in range(8):\n",
        "    print(f'Epoch {epoch}')\n",
        "    train_epoch(classifier,optim,train_loader,loss_fn=nn.CrossEntropyLoss())#weight=weight))\n",
        "    _=loader_stats(classifier,train_loader)\n",
        "    _=loader_stats(classifier,validation_loader)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Loss 0.1684\n",
            "{'acc': 0.9404387419013504,\n",
            " 'f1': 0.9509222922645543,\n",
            " 'nr': 0.9689058288030279,\n",
            " 'pr': 0.9233529185816298,\n",
            " 'prec': 0.980188661543339}\n",
            "{'acc': 0.9366343831745019,\n",
            " 'f1': 0.9477794221511937,\n",
            " 'nr': 0.9650660155467603,\n",
            " 'pr': 0.9195975470751347,\n",
            " 'prec': 0.9777432225573773}\n",
            "Epoch 1\n",
            "Loss 0.0728\n",
            "{'acc': 0.9704879378499783,\n",
            " 'f1': 0.976121811258798,\n",
            " 'nr': 0.9791946653037529,\n",
            " 'pr': 0.9652621974192152,\n",
            " 'prec': 0.987228556057106}\n",
            "{'acc': 0.962483059327457,\n",
            " 'f1': 0.9695925127141365,\n",
            " 'nr': 0.9723586317685425,\n",
            " 'pr': 0.9565654072256253,\n",
            " 'prec': 0.9829793396407629}\n",
            "Epoch 2\n",
            "Loss 0.0197\n",
            "{'acc': 0.9815225953777265,\n",
            " 'f1': 0.9851454233390339,\n",
            " 'nr': 0.9833146062580046,\n",
            " 'pr': 0.9804470381423411,\n",
            " 'prec': 0.9898890554820376}\n",
            "{'acc': 0.9712865939592605,\n",
            " 'f1': 0.9768746115975243,\n",
            " 'nr': 0.9736524830336973,\n",
            " 'pr': 0.9698689031329754,\n",
            " 'prec': 0.9839822659072608}\n",
            "Epoch 3\n",
            "Loss 0.0306\n",
            "{'acc': 0.9863430273980858,\n",
            " 'f1': 0.9890280119422475,\n",
            " 'nr': 0.9886389506284726,\n",
            " 'pr': 0.9849650241339823,\n",
            " 'prec': 0.9931246583001307}\n",
            "{'acc': 0.9742615055369664,\n",
            " 'f1': 0.9792819637862629,\n",
            " 'nr': 0.9767106769331545,\n",
            " 'pr': 0.9727939101801544,\n",
            " 'prec': 0.9858571426836223}\n",
            "Epoch 4\n",
            "Loss 0.0019\n",
            "{'acc': 0.9883510816511383,\n",
            " 'f1': 0.9906311407242419,\n",
            " 'nr': 0.9931113998996657,\n",
            " 'pr': 0.9854939590793452,\n",
            " 'prec': 0.9958221612079369}\n",
            "{'acc': 0.9739309598061102,\n",
            " 'f1': 0.9789733743987155,\n",
            " 'nr': 0.979621842279753,\n",
            " 'pr': 0.9705208625350574,\n",
            " 'prec': 0.9875744099234115}\n",
            "Epoch 5\n",
            "Loss 0.0218\n",
            "{'acc': 0.9870454332067666,\n",
            " 'f1': 0.9895505839728622,\n",
            " 'nr': 0.996203191596172,\n",
            " 'pr': 0.9815489859451805,\n",
            " 'prec': 0.9976837124669418}\n",
            "{'acc': 0.9717934307465734,\n",
            " 'f1': 0.9771603945266454,\n",
            " 'nr': 0.9832093389695007,\n",
            " 'pr': 0.9649527768307891,\n",
            " 'prec': 0.9896808470417172}\n",
            "Epoch 6\n",
            "Loss 0.0059\n",
            "{'acc': 0.9947608789719246,\n",
            " 'f1': 0.9958011020013599,\n",
            " 'nr': 0.9958286515094218,\n",
            " 'pr': 0.9941200064799715,\n",
            " 'prec': 0.9974878927491664}\n",
            "{'acc': 0.9783823089816404,\n",
            " 'f1': 0.9826405476871263,\n",
            " 'nr': 0.978239773882883,\n",
            " 'pr': 0.9784677190307063,\n",
            " 'prec': 0.9868491201397372}\n",
            "Epoch 7\n",
            "Loss 0.0350\n",
            "{'acc': 0.9919705374652823,\n",
            " 'f1': 0.9935424415114859,\n",
            " 'nr': 0.9978629182551041,\n",
            " 'pr': 0.9884339558173205,\n",
            " 'prec': 0.9987040055136589}\n",
            "{'acc': 0.9743165964921091,\n",
            " 'f1': 0.9792724457876124,\n",
            " 'nr': 0.9810921278083381,\n",
            " 'pr': 0.9702565546693485,\n",
            " 'prec': 0.9884574647751782}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBflr8j2834p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "def generate_csv(model,embeds,filename='test.csv',thr=0.5):\n",
        "    predictions = []\n",
        "    with open(\"testing.txt\", \"r\") as f:\n",
        "        for l in f:\n",
        "            l = [int(x) for x in l.split()]\n",
        "            x=torch.cat([embeds[l[0]],embeds[l[1]]])\n",
        "            x=model(x)[1]\n",
        "            if x>thr:\n",
        "                predictions.append(\"1\")\n",
        "            else: \n",
        "                predictions.append(\"0\")\n",
        "            \n",
        "    \n",
        "    predictions = zip(range(len(predictions)), predictions)\n",
        "    # Write the output in the format required by Kaggle\n",
        "    with open(filename,\"w\") as pred:\n",
        "        csv_out = csv.writer(pred)\n",
        "        csv_out.writerow(['id','predicted'])\n",
        "        for row in predictions:\n",
        "            csv_out.writerow(row) \n",
        "generate_csv(classifier,embed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTn6XYJP3OFn",
        "colab_type": "text"
      },
      "source": [
        "classifier=nn.Sequential(nn.Linear(2*d,2*d),nn.ReLU(), nn.Linear(2*d,d),nn.ReLU(),nn.Linear(d,2)).cuda()\n",
        "\n",
        "optim=torch.optim.Adam(classifier.parameters(),lr=1e-5)\n",
        "\n",
        "optim=torch.optim.Adam(classifier.parameters(),lr=1e-5)"
      ]
    }
  ]
}